# LLM Multi-Provider System Configuration
# Copy this file to .env in your project root and fill in your API keys

# ============================================================================
# Provider Selection
# ============================================================================
# Choose your default provider: gemini, openai, anthropic, ollama
VITE_LLM_PROVIDER=gemini

# ============================================================================
# Google Gemini Configuration
# ============================================================================
# Get your API key from: https://makersuite.google.com/app/apikey
VITE_GEMINI_API_KEY=your_gemini_api_key_here
VITE_GEMINI_MODEL=gemini-1.5-flash

# ============================================================================
# OpenAI Configuration
# ============================================================================
# Get your API key from: https://platform.openai.com/api-keys
VITE_OPENAI_API_KEY=your_openai_api_key_here
VITE_OPENAI_MODEL=gpt-4-turbo
VITE_OPENAI_ORGANIZATION=your_org_id_optional

# ============================================================================
# Anthropic Claude Configuration
# ============================================================================
# Get your API key from: https://console.anthropic.com/
VITE_ANTHROPIC_API_KEY=your_anthropic_api_key_here
VITE_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ============================================================================
# Ollama Local Configuration
# ============================================================================
# No API key required - runs locally
# Install from: https://ollama.ai/
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_OLLAMA_MODEL=llama2

# ============================================================================
# Common Settings
# ============================================================================
# Temperature: 0.0 (deterministic) to 2.0 (very creative)
VITE_LLM_TEMPERATURE=0.7

# Maximum tokens in response (higher = longer but more expensive)
VITE_LLM_MAX_TOKENS=2048

# ============================================================================
# Model Recommendations
# ============================================================================
#
# Fast & Cheap:
#   - gemini-1.5-flash (Google)
#   - gpt-3.5-turbo (OpenAI)
#   - claude-3-haiku (Anthropic)
#
# Balanced:
#   - gemini-1.5-pro (Google)
#   - gpt-4-turbo (OpenAI)
#   - claude-3-5-sonnet (Anthropic)
#
# Most Capable:
#   - gemini-1.5-pro (Google)
#   - gpt-4 (OpenAI)
#   - claude-3-opus (Anthropic)
#
# Local (Free):
#   - llama2 (Ollama)
#   - mistral (Ollama)
#   - codellama (Ollama)
#
# ============================================================================
